import json
import os
import pinecone
from operator import itemgetter
from langchain.vectorstores import Pinecone
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser

# Enter your API Keys and Pinecone index name below
PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'Your Pinecone API Key')
PINECONE_API_ENV = os.getenv('PINECONE_API_ENV', 'Your Pinecone API Environment, e.g. gcp-starter')
index_name = "Your index name of the uploaded pdf file in Pinecone"
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'Your OpenAI API Key')

def get_vectorstore(PINECONE_API_KEY, PINECONE_API_ENV, OPENAI_API_KEY, index_name):
    """
    Get the vector store of Pinecone from uploaded existing indexes
    Args:
        PINECONE_API_KEY (`PineconeAPIKey`):
            The Pinecone API Key created from your account.
        PINECONE_API_ENV (`PineconeAPIEnvironment`):
            The Pinecone API Environment that you selected in the uploaded Pinecone index.
        OPENAI_API_KEY (`OpenAIAPIKey`):
            The OpenAI API Key created from your account.
        index_name (`IndexName`):
            The name of your uploaded Pinecone index.

    Returns:
        `VectorStore`: The vector store that created from your existing Pinecone Indexes.
    """
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)
    vectorstore = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)
    return vectorstore

def get_response(OPENAI_API_KEY, vectorstore, user_prompt):
    """
    Get response of ChatOpenAI with OpenAI API from user prompt
    Args:
        OPENAI_API_KEY (`OpenAIAPIKey`):
            The OpenAI API Key created from your account.
        vectorstore (`VectorStore`):
            The vector store that you created in Pinecone.
        user_prompt (`UserPrompt`):
            The question and instructions from the users.

    Returns:
        `response`: The response that generated by ChatOpenAI from the user prompt.
    """

    retriever = vectorstore.as_retriever()
    model = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
    
    # The template is used to control the system prompt
    template = """Answer the question based only on the following context:
    {context}
    Question: {question}
    Answer in the following language: {language}
    """
    prompt = ChatPromptTemplate.from_template(template)
    chain = (
        {
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "language": itemgetter("language"),
        }
        | prompt
        | model
        | StrOutputParser()
    )
    response = chain.invoke({"question": user_prompt, "language": "english"})  # You may change the response language here, default: English

    return response

def lambda_handler(event, context):
    try:
        request_body = event.get('body')
        request_body = json.loads(request_body)
        user_prompt = request_body.get('userprompt')

        # Validate if 'userprompt' is present
        if user_prompt is None:
            raise ValueError(f"'userprompt' is missing in the request JSON., {event}")

        # Get the PDF embeddings from Pinecone
        vectorstore = get_vectorstore(PINECONE_API_KEY, PINECONE_API_ENV, OPENAI_API_KEY, index_name)
        # Generate response with OpenAI API
        query_response = get_response(OPENAI_API_KEY, vectorstore, user_prompt) 

        response = {
            'statusCode': 200,
            'body': query_response
        }

    except Exception as e:
        # Handle exceptions and return the content of exception as string
        response = {
            'statusCode': 400,
            'body': json.dumps({'error': str(e)})
        }

    return response